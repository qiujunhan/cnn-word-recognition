{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from  PIL  import   Image,ImageFont,ImageDraw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_photo = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除数据集\n",
    "for i in os.scandir(r\"train_data\"):\n",
    "    if os.path.isfile(i.path):\n",
    "        os.remove(i.path)\n",
    "for i in os.scandir(r\"test_data\"):\n",
    "    if os.path.isfile(i.path):\n",
    "        os.remove(i.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"text.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    text_list = eval(text)\n",
    "\n",
    "# text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成数据集\n",
    "i = 0\n",
    "\n",
    "for text in text_list:\n",
    "    if i > max_photo:\n",
    "        break\n",
    "    if len(text) > 1:\n",
    "        continue\n",
    "    im = Image.new(\"RGB\", (32, 32), (255, 255, 255))\n",
    "    dr = ImageDraw.Draw(im)\n",
    "    font = ImageFont.truetype(os.path.join( \"FZZJ-JHTJW.TTF\"), 32)\n",
    "    dr.text((0, 0), text, font=font, fill=\"#000000\")\n",
    "    im.save(\"train_data/{}_{}.png\".format(text,i))\n",
    "    im.save(\"test_data/{}_{}.png\".format(text,i))\n",
    "    \n",
    "    \n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "text_json = {}\n",
    "for text in text_list:\n",
    "    if i > max_photo:\n",
    "        break\n",
    "    if len(text) > 1:\n",
    "        continue\n",
    "    text_json[i] = text \n",
    "    i+= 1\n",
    "\n",
    "n_classfication = i\n",
    "\n",
    "with open (\"text_json.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(str(text_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self,transform=None,train=True):\n",
    "        \n",
    "\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.img_path  = []\n",
    "        if train:\n",
    "\n",
    "            self.img_path = self.get_img_path_list(True)\n",
    "        else:\n",
    "            self.img_path = self.get_img_path_list(False)\n",
    "\n",
    "\n",
    "        self.img_len = len(self.img_path)\n",
    "\n",
    "    def get_img_path_list(self,istrain):\n",
    "        def train():\n",
    "            draw_list = []\n",
    "            \n",
    "            for i in os.scandir(r\"train_data\"):\n",
    "                if i.name == r\".ipynb_checkpoints\":\n",
    "                    continue\n",
    "                for j in range(10):\n",
    "                    draw_list.append(i.path)\n",
    "            return draw_list\n",
    "\n",
    "\n",
    "        def test():\n",
    "            draw_list = []\n",
    "            \n",
    "            for i in os.scandir(r\"test_data\"):\n",
    "                if i.name == r\".ipynb_checkpoints\":\n",
    "                    continue\n",
    "                for j in range(10):\n",
    "                    draw_list.append(i.path)\n",
    "            return draw_list\n",
    "        if istrain :\n",
    "            return train()\n",
    "        else:\n",
    "            return test()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        path =  self.img_path[idx]\n",
    "        label = re.findall(r'/.*?_(\\d+).png',path)\n",
    "        \n",
    "\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "\n",
    "        label = torch.from_numpy(np.array([label]).astype(np.int))\n",
    "        \n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,classfication,pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = models.resnet18(pretrained=pretrained,num_classes=classfication)\n",
    "        self.name = \"resnet18\"\n",
    "\n",
    "\n",
    "        \n",
    "        def inplace_relu(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find('ReLU') != -1:\n",
    "                m.inplace = True\n",
    "\n",
    "        self.model.apply(inplace_relu)\n",
    "\n",
    "    def forward(self,image):\n",
    "        x = self.model(image)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_epochs = 5\n",
    "b1 = 0.9\n",
    "b2 = 0.999\n",
    "n_cpu =4\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),  # 将图像转为Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "])\n",
    "test_image_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),  # 将图像转为Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 标准化\n",
    "])\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    ImgDataset(\n",
    "               train=True,\n",
    "               transform=train_image_transform,\n",
    "               \n",
    "               ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=n_cpu\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    ImgDataset(\n",
    "               train=False,\n",
    "               transform=test_image_transform,\n",
    "               ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=n_cpu,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.optim import SGD\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import os\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import MSELoss\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "# from torch.utils.tensorboard import Summarywriter\n",
    "import random\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "def accuracy(output, target, topk=(1, 2)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        if k <= 0 :\n",
    "            raise ValueError(\"k值错误\")\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      " 9.31247329711914 0.21484375\n",
      " 8.701658248901367 0.224609375\n",
      " 8.477664947509766 0.2734375\n",
      " 7.709378719329834 0.3515625\n",
      " 6.839502334594727 0.62890625\n",
      " 5.833504676818848 1.357421875\n",
      " 5.085869312286377 2.6729910714285716\n",
      " 3.927508592605591 4.70703125\n",
      " 3.130091905593872 7.528211805555555\n",
      "测试集prec1: 53.09681056701031\n",
      "epoch:1\n",
      " 1.8253791332244873 58.96484375\n",
      " 1.2702912092208862 62.900390625\n",
      " 0.8993560671806335 67.0703125\n",
      " 0.7090749144554138 70.9423828125\n",
      " 0.5311341881752014 74.19140625\n",
      " 0.435355007648468 76.93359375\n",
      " 0.2663225829601288 79.25502232142857\n",
      " 0.23724068701267242 81.19873046875\n",
      " 0.24258726835250854 82.83203125\n",
      "测试集prec1: 97.57168170103093\n",
      "epoch:2\n",
      " 0.18929676711559296 96.97265625\n",
      " 0.13876818120479584 97.08984375\n",
      " 0.12978331744670868 97.29166666666667\n",
      " 0.09427209943532944 97.3876953125\n",
      " 0.07621195167303085 97.51171875\n",
      " 0.05900627374649048 97.66276041666667\n",
      " 0.09313693642616272 97.74553571428571\n",
      " 0.08387373387813568 97.84912109375\n",
      " 0.052906669676303864 97.90364583333333\n",
      "测试集prec1: 99.1100193298969\n",
      "epoch:3\n",
      " 0.12874218821525574 98.57421875\n",
      " 0.1114741712808609 98.69140625\n",
      " 0.08081778883934021 98.6328125\n",
      " 0.08691678196191788 98.5205078125\n",
      " 0.050699565559625626 98.4921875\n",
      " 0.0917380303144455 98.50911458333333\n",
      " 0.05170203745365143 98.52678571428571\n",
      " 0.08532743901014328 98.53515625\n",
      " 0.07576733827590942 98.53949652777777\n",
      "测试集prec1: 99.18250644329896\n",
      "epoch:4\n",
      " 0.07984675467014313 98.7109375\n",
      " 0.07923247665166855 98.73046875\n",
      " 0.04555680975317955 98.76953125\n",
      " 0.06893158704042435 98.75\n",
      " 0.049142200499773026 98.76953125\n",
      " 0.08319894224405289 98.74674479166667\n",
      " 0.05835011973977089 98.71930803571429\n",
      " 0.07598477602005005 98.701171875\n",
      " 0.05699754133820534 98.70225694444444\n",
      "测试集prec1: 98.90665270618557\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Model(n_classfication)\n",
    "model_optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "\n",
    "cel = CrossEntropyLoss()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    cel.cuda()\n",
    "model.zero_grad()\n",
    "for _epoch in range(n_epochs):\n",
    "\n",
    "    print(\"epoch:{}\".format(_epoch))\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "    sum_pre1 = 0\n",
    "    for idx, (image, label) in enumerate(train_dataloader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device, dtype=torch.long).view(batch_size,-1)\n",
    "        predict = model(image.float())\n",
    "        loss = cel(predict, label.squeeze())\n",
    "        predicted = predict\n",
    "        \n",
    "\n",
    "        prec1, prec5 = accuracy(predict.data, label.data, topk=(1, 5))\n",
    "        sum_pre1 += prec1\n",
    "        loss.backward()\n",
    "        model_optimizer.step()\n",
    "        model_optimizer.zero_grad()\n",
    "        if (idx+1) %20 ==0:\n",
    "            \n",
    "            print(\"\",loss.item(),sum_pre1.item()/(idx+1))\n",
    "           \n",
    "    sum_pre1 = 0\n",
    "    for idx, (test_image, test_label) in enumerate(test_dataloader):\n",
    "        test_image = test_image.to(device)\n",
    "        test_label = test_label.to(device, dtype=torch.long)\n",
    "        \n",
    "        predict = model(test_image.float()).detach()\n",
    "        prec1, prec5 = accuracy(predict.data, test_label.data, topk=(1, 5))\n",
    "        sum_pre1 += prec1\n",
    "        predicted = predict\n",
    "        \n",
    "        d_loss = cel(predict, test_label.squeeze())\n",
    "    \n",
    "    print(\"测试集prec1:\",(sum_pre1.item())/idx)\n",
    "    torch.save(model, \"model\", _use_new_zipfile_serialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寰\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from  PIL import Image\n",
    "\n",
    "model =  torch.load(\"model\")\n",
    "model.eval()\n",
    "with open (\"text_json.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    text_json = f.read()\n",
    "    text_json = eval(text_json)\n",
    "    \n",
    "path = \"test_data/寰_2074.png\"\n",
    "str_ = re.findall(\"(.*?)_\\d+.png\",path)[0]\n",
    "\n",
    "image1 = Image.open(path).convert('RGB')\n",
    "image = test_image_transform(image1).to(device)\n",
    "image_ = image.view(1,3,32,32)\n",
    "pred = model(image_.float())\n",
    "pred = pred.cpu()\n",
    "plt.imshow(image1)\n",
    "plt.show()\n",
    "\n",
    "print(text_json[pred.argmax().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-794c039907d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_label' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_label[:10])\n",
    "print(torch.argmax(predict,dim=1)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
